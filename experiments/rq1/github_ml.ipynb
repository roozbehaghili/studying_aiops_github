{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import csv\n",
        "import time\n",
        "import requests\n",
        "import pandas as pd\n",
        "from random import randint\n",
        "from datetime import datetime"
      ],
      "metadata": {
        "id": "Qh2MMzF5Cmrn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aTKOxaPheyyN"
      },
      "outputs": [],
      "source": [
        "# Gather all the project names in Machine Learning and Deep Learning topics\n",
        "now = time.time()\n",
        "\n",
        "# list of tokens for authorizing github api\n",
        "token = ['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', # Rooz\n",
        "         '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', #Xing\n",
        "         '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', #Ahura\n",
        "         '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' #Rooz\n",
        "         ]\n",
        "\n",
        "# initializing values. set the time period, topic and output path here\n",
        "projects = 0\n",
        "intervals = 30\n",
        "topic = ['deep-learning'] # Run this code once with 'machine-learning' and once with 'deep-learning' keywords\n",
        "output_path = '/content/deep-projects'\n",
        "df = open(output_path +'.csv', mode='w', newline='', encoding='utf-8')\n",
        "\n",
        "# create csv files as the number of topics\n",
        "for topic in topic:\n",
        "    # set the start and final date here\n",
        "    start_date = datetime.datetime.strptime('12-12-25', '%y-%m-%d')\n",
        "    final_date = datetime.datetime.strptime('22-10-27', '%y-%m-%d')\n",
        "    next_date = start_date + datetime.timedelta(intervals)\n",
        "    days = abs((final_date - start_date).days)\n",
        "    # divide the whole period to interval days\n",
        "    for day in range(int(days / intervals) + 1):\n",
        "        header = {'Authorization': 'token ' + token[day % len(token)]}\n",
        "        for p in range(1,11):\n",
        "            # create and send the request\n",
        "            my_request = ('https://api.github.com/search/repositories?q=' + topic + '+stars:>=1+forks:>=1+created:' + \n",
        "                            str(start_date)[0:10] + '..' + str(next_date)[0:10] + '&sort=updated&order=desc&page=' + str(p) + '&per_page=100')\n",
        "            req = requests.get(my_request, headers=header)\n",
        "\n",
        "            # check if the request is ok\n",
        "            if(req.status_code == 200):\n",
        "                req2 = req.json()['items']\n",
        "\n",
        "                # write the name of the repositories to the file\n",
        "                for i in range(len(req2)):\n",
        "                    df.write(req2[i]['full_name'] + '\\n')\n",
        "                    projects += 1\n",
        "                    \n",
        "                    # set a sleep time for not getting banned by github api\n",
        "                    if projects % 1000 == 0:\n",
        "                        print(projects)\n",
        "                        time.sleep(5)\n",
        "\n",
        "                # break the for statement if there isn't more projects in the page\n",
        "                if len(req2) != 100:\n",
        "                    break\n",
        "\n",
        "            # pass the unwanted status codes\n",
        "            else:\n",
        "                print(req.status_code, ' || ',  req.json())\n",
        "\n",
        "        # break if the period is finished\n",
        "        if next_date == final_date:\n",
        "            break\n",
        "\n",
        "        # set and configure start_date and next_date for the next interval.\n",
        "        start_date = next_date + datetime.timedelta(1)\n",
        "        next_date = start_date + datetime.timedelta(intervals)\n",
        "\n",
        "        # change the next_date if it passes the final date\n",
        "        if next_date > final_date:\n",
        "            next_date = final_date\n",
        "df.close()\n",
        "print('number of projects: ', projects)\n",
        "later = time.time()\n",
        "print(f'final time: {later - now:.2f} seconds')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove duplicate projects\n",
        "ml_projects = pd.read_csv('/content/ml-projects.csv', header=None)[0]\n",
        "deep_projects = pd.read_csv('/content/deep-projects.csv', header=None)[0]\n",
        "ml_deep = pd.concat([ml_projects, deep_projects])\n",
        "\n",
        "print(len(ml_projects) + len(deep_projects))\n",
        "print(len(ml_deep))\n",
        "ml_deep = ml_deep.drop_duplicates()\n",
        "print(len(ml_deep))\n",
        "ml_deep.to_csv('ml-deep-projects.csv', index=False)"
      ],
      "metadata": {
        "id": "A5ud0ccNUKCF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Randomly select projects and extract their metrics\n",
        "# list of tokens for authorizing github api\n",
        "token = ['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', # Rooz\n",
        "         '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', #Xing\n",
        "         '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', #Ahura\n",
        "         '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' #Rooz\n",
        "         ]\n",
        "\n",
        "def random_projects(input_path, repos_path):\n",
        "    # this method randomly select 383 repository names from the whole list of names\n",
        "    df = pd.read_csv(input_path, header=None)[0]\n",
        "    repos =  pd.Series()\n",
        "    for i in range(500):\n",
        "        value = randint(0, len(df))\n",
        "        # repos = repos.append([df[value]])\n",
        "        repos = pd.concat([repos, pd.Series(df[value])])\n",
        "    repos.to_csv(repos_path, header=False, index=False)\n",
        "    \n",
        "def get_metrics(repo_name):\n",
        "  # this method retrieves github metrics such as num of\n",
        "  # releases, pulls, and issues from the repositories.\n",
        "  repo = 'https://github.com/' + repo_name\n",
        "  branch = 'main'\n",
        "\n",
        "  response = requests.get(f'{repo}')\n",
        "  response_pulls = requests.get(f'{repo}/pulls')\n",
        "  response_issues = requests.get(f'{repo}/issues')\n",
        "  commits = re.findall(r'<strong>(.*?)</strong>\\n(\\s*?)<span aria-label=\"Commits', response.content.decode('utf-8'), re.MULTILINE)\n",
        "  contributors = re.findall(r'Contributors <span title=\"(.*?)\"', response.content.decode('utf-8'), re.MULTILINE)\n",
        "  releases = re.findall(r'Releases\\n(\\s*?)<span title=\"(.*?)\"', response.content.decode('utf-8'), re.MULTILINE)\n",
        "  branches = re.findall(r'<strong>(.*?)</strong>\\n(\\s*?)<span class=\"color-fg-muted\">branches</span>', response.content.decode('utf-8'), re.MULTILINE)\n",
        "  open_pulls = re.findall(r'</svg>\\n(\\s*?)(.*?) Open', response_pulls.content.decode('utf-8'), re.MULTILINE)\n",
        "  closed_pulls = re.findall(r'</svg>\\n(\\s*?)(.*?) Closed', response_pulls.content.decode('utf-8'), re.MULTILINE)\n",
        "  open_issues = re.findall(r'</svg>\\n(\\s*?)(.*?) Open', response_issues.content.decode('utf-8'), re.MULTILINE)\n",
        "  closed_issues = re.findall(r'</svg>\\n(\\s*?)(.*?) Closed', response_issues.content.decode('utf-8'), re.MULTILINE)\n",
        "\n",
        "  try:\n",
        "    commits = commits[0][0]\n",
        "  except:\n",
        "    commits = 0\n",
        "  try:\n",
        "    contributors = contributors[0]\n",
        "  except:\n",
        "    contributors = 0\n",
        "  try:\n",
        "    releases = releases[0][1]\n",
        "  except:\n",
        "    releases = 0\n",
        "  try:\n",
        "    branches = branches[0][0]\n",
        "  except:\n",
        "    branches = 0\n",
        "  try:\n",
        "    open_pulls = open_pulls[0][1].replace(' ', '')\n",
        "  except:\n",
        "    open_pulls = 0\n",
        "  try:\n",
        "    closed_pulls = closed_pulls[0][1].replace(' ', '')\n",
        "  except:\n",
        "    closed_pulls = 0\n",
        "  try:\n",
        "    open_issues = open_issues[0][1].replace(' ', '')\n",
        "  except:\n",
        "    open_issues = 0\n",
        "  try:\n",
        "    closed_issues = closed_issues[0][1].replace(' ', '')\n",
        "  except:\n",
        "    closed_issues = 0\n",
        "\n",
        "  return commits, contributors, releases, branches, open_pulls, closed_pulls, open_issues, closed_issues\n",
        "\n",
        "# initializing values. set the input path here\n",
        "input_path = '/content/ml-deep-projects.csv'\n",
        "total_proj = 0\n",
        "\n",
        "# create 1 csv files, each have 383 repository metrics - you can have more projects by changing the range\n",
        "for i in range(1):\n",
        "    # set the repos and output path here\n",
        "    repos_path = ('/content/repo_names/' + str(i) + '.csv')\n",
        "    output_path = ('/content/repo_details/' + str(i) + '.csv')\n",
        "\n",
        "    # rendomly select some projects\n",
        "    random_projects(input_path, repos_path)\n",
        "\n",
        "    repo_names = pd.read_csv(repos_path, header=None)[0].tolist()\n",
        "    file = open(output_path, mode='w', newline='', encoding='utf-8')  \n",
        "    writer = csv.writer(file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
        "    writer.writerow(['Repos', 'id', 'stars', 'forks', 'commits', 'contributors','releases', 'branches', 'open_pulls','closed_pulls',\n",
        "                        'open_issues','closed_issues','size', 'archived', 'created_at', 'updated_at', 'age', 'language'])\n",
        "    header = {'Authorization': 'token ' + token[i % len(token)]}\n",
        "\n",
        "    for repo in repo_names:\n",
        "        # create and send the request\n",
        "        req = requests.get('https://api.github.com/repos/' + repo, headers=header)\n",
        "        \n",
        "        # check if the request is ok\n",
        "        if(req.status_code == 200):\n",
        "            req = req.json()\n",
        "            commits, contributors, releases, branches, open_pulls, closed_pulls, open_issues, closed_issues = get_metrics(req['full_name'])\n",
        "            date_created = datetime.strptime(req['created_at'][2:10], '%y-%m-%d')\n",
        "            date_updated = datetime.strptime(req['updated_at'][2:10], '%y-%m-%d')\n",
        "            age = (date_updated.year - date_created.year) * 12 + (date_updated.month - date_created.month)\n",
        "            \n",
        "            # write the metrics into the file\n",
        "            writer.writerow([req['full_name'], req['id'], req['stargazers_count'], req['forks'], commits, contributors, releases, branches, open_pulls, \n",
        "                            closed_pulls, open_issues, closed_issues, req['size'], req['archived'], date_created, date_updated, age, req['language']])\n",
        "            file.flush()\n",
        "            total_proj += 1\n",
        "\n",
        "            # set a sleep time for not getting banned by github api\n",
        "            if total_proj % 10 == 0:\n",
        "                time.sleep(5)\n",
        "\n",
        "        # pass the unwanted status codes        \n",
        "        else:\n",
        "            print(req.status_code, ' || ',  req.json())"
      ],
      "metadata": {
        "id": "WuAn83CACAkb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloading files\n",
        "!zip -r /content/repo_details.zip /content/repo_details\n",
        "!zip -r /content/repo_names.zip /content/repo_names\n",
        "\n",
        "from google.colab import files\n",
        "files.download('/content/repo_details.zip')\n",
        "files.download('/content/repo_names.zip')"
      ],
      "metadata": {
        "id": "Xu1nFrpGJEsv"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}